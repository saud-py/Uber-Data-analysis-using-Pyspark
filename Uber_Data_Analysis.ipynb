{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGNXq9vTp3hOsO2kGchP++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saud-py/Uber-Data-analysis-using-Pyspark/blob/main/Uber_Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Uber Data Analysis in Pyspark"
      ],
      "metadata": {
        "id": "sU8o26QSgQuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use Uber dataset to perform analysis to gain data insights fronm city supply and demand data by cleaning and querying the data to feature engineering"
      ],
      "metadata": {
        "id": "M-qezyCVgWu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to gain insights into the demand and supply industry by cleaning, transforming and analyzing the data using PySpark"
      ],
      "metadata": {
        "id": "8EPnNbfyiJMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that during the hour beginning at 4 pm (hour 16), on September 10th, 2012, 11 people opened the Uber app (Eyeballs). 2 of them did not see any car (Zeroes) and 4 of them requested a car (Requests). Of the 4 requests, only 3 complete trips actually resulted (Completed Trips). During this time, there were a total of 6 drivers who logged in (Unique Drivers)."
      ],
      "metadata": {
        "id": "o-cwqjLDicHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requirements to be satisfied\n",
        "\n",
        "1) Which date had the most completed trips during the two-week period?\n",
        "\n",
        "2) What was the highest number of completed trips within a 24-hour period?\n",
        "\n",
        "3) Which hour of the day had the most requests during the two-week period?\n",
        "\n",
        "4) What percentages of all zeroes during the two-week period occurred on weekend (Friday at 5 pm to Sunday at 3 am)? Tip: The local time value is the start of the hour (e.g. 15 is the hour from 3:00 pm - 4:00 pm)\n",
        "\n",
        "5) What is the weighted average ratio of completed trips per driver during the two-week period? Tip: “Weighted average” means your answer should account for the total trip volume in each hour to determine the most accurate number in the whole period.\n",
        "\n",
        "6) In drafting a driver schedule in terms of 8 hours shifts, when are the busiest 8 consecutive hours over the two-week period in terms of unique requests? A new shift starts every 8 hours. Assume that a driver will work the same shift each day.\n",
        "\n",
        "7) True or False: Driver supply always increases when demand increases during the two-week period. Tip: Visualize the data to confirm your answer if needed.\n",
        "\n",
        "8) In which 72-hour period is the ratio of Zeroes to Eyeballs the highest?\n",
        "\n",
        "9) If you could add 5 drivers to any single hour of every day during the two-week period, which hour should you add them to? Hint: Consider both rider eyeballs and driver supply when choosing\n",
        "\n",
        "10) Looking at the data from all two weeks, which time might make the most sense to consider a true “end day” instead of midnight? (i.e when are supply and demand at both their natural minimums)"
      ],
      "metadata": {
        "id": "f720IF-biw8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First to perform the task we need inialize a spark session\n",
        "!pip install pyspark py4j\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "b83MK3AxjEcI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc28662-a646-4a22-a3a4-c78e792d447b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=2c71f3e8df358133f992d64d570eb1f9f05ad1b1cc02ec8060b016b6bfa513f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the spark\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#Creating a SparkSession\n",
        "spark = SparkSession.builder.appName(\"UberDataAnalysis\").getOrCreate()\n",
        "\n",
        "#loading the dataset into dataframe\n",
        "df = spark.read.csv(\"uber.csv\", header = True, inferSchema = True)"
      ],
      "metadata": {
        "id": "ciWQ-tc-jtMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWlgwba3taR0",
        "outputId": "83b84732-1e74-48d7-f54e-a360ce579238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Date',\n",
              " 'Time',\n",
              " 'Eyeballs ',\n",
              " 'Zeroes ',\n",
              " 'Completed',\n",
              " 'Requests ',\n",
              " 'Unique Drivers']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Which date had the most completed trips during the two week period?\n",
        "\n",
        "To solve we need to the group the data by date and sum up the total trips completed. Then the sort the results in desc order, select the top most rows"
      ],
      "metadata": {
        "id": "mKfzC16ppCgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import max\n",
        "\n",
        "completed_trips_by_date = df.groupBy(\"Date\").sum(\"Completed\")\n",
        "\n",
        "date_with_most_completed_trips = completed_trips_by_date \\\n",
        ".orderBy(\"sum(Completed)\", ascending = False) \\\n",
        ".select(\"Date\") \\\n",
        ".first()[\"Date\"]\n",
        "\n",
        "\n",
        "print(date_with_most_completed_trips)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIa-3yTtoTXe",
        "outputId": "3ed034ba-9898-48b9-9aeb-5ed2ccbced17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Which was the highest number of completed trips within 24hr period?\n",
        "\n",
        "To find the highest number of completed trips within a 24hr period, we can group the data by date and use a window function to sum the completed trips column over a rolling 24hr period. Then, we can sort the results in descending order and select the top row\n"
      ],
      "metadata": {
        "id": "0Bi_L0Bs0pCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, window\n",
        "\n",
        "#This will group the data and sum it together the total trips within 24hr\n",
        "completed_trips_by_window = df \\\n",
        ".groupBy(window(\"Time\", \"24 hours\")) \\\n",
        ".agg(sum(\"Completed\").alias(\"Total Completed Trips\")) \\\n",
        ".orderBy(\"Total Completed Trips\", ascending = False)\n",
        "\n",
        "#To get the highest number trips completed in 24hrs\n",
        "highest_completed_trips = completed_trips_by_window \\\n",
        ".select(\"Total Completed Trips\") \\\n",
        ".first()[\"Total Completed Trips\"]\n",
        "\n",
        "print(highest_completed_trips)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "LHo3Nzn41Vr9",
        "outputId": "b65efb79-4601-4ef1-d5de-fd3ce4beb6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-2261834f7ea0>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    .orderBy(\"Total Completed Trips\", ascending = False)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "8S5EU2JF7U1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1thsMMbaa2CJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}